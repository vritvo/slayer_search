{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Meta Evaluation Analysis\n",
    "\n",
    "This notebook analyzes evaluation results across different model configurations.\n",
    "\n",
    "## Setup\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import glob\n",
    "import os\n",
    "import toml\n",
    "import numpy as np\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import shared calculation function\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "from evaluate_semantic_search import calculate_evaluation_metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "Load the most recent evaluation file or specify a different one.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Auto-detect most recent evaluation file\n",
    "eval_files = glob.glob(\"evals/evaluation_*.csv\")\n",
    "if not eval_files:\n",
    "    raise FileNotFoundError(\"No evaluation files found in eval/ directory.\")\n",
    "\n",
    "eval_path = max(eval_files, key=os.path.getmtime)\n",
    "print(f\"Loading: {eval_path}\")\n",
    "print(f\"File size: {os.path.getsize(eval_path) / (1024*1024):.1f} MB\")\n",
    "\n",
    "# Load evaluation data\n",
    "eval_df = pd.read_csv(eval_path)\n",
    "\n",
    "# Fill NaN values\n",
    "eval_df[\"notes\"] = eval_df[\"notes\"].fillna(\"\")\n",
    "eval_df[\"db_tag\"] = eval_df[\"db_tag\"].fillna(\"\")\n",
    "\n",
    "print(f\"\\nTotal rows: {len(eval_df):,}\")\n",
    "print(f\"Date range: {eval_df['date'].min()} to {eval_df['date'].max()}\")\n",
    "\n",
    "# Show models found\n",
    "unique_models = eval_df[[\"bi_encoder_model\", \"db_tag\", \"meta_data_included\"]].drop_duplicates()\n",
    "print(f\"\\nFound {len(unique_models)} model configurations:\")\n",
    "for _, row in unique_models.iterrows():\n",
    "    meta_str = \"with meta\" if row['meta_data_included'] else \"no meta\"\n",
    "    tag_str = f\" ({row['db_tag']})\" if row['db_tag'] else \"\"\n",
    "    print(f\"  - {row['bi_encoder_model']}{tag_str} - {meta_str}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Metrics\n",
    "\n",
    "Calculate metrics per model configuration using the shared `calculate_meta_evaluation_metrics` function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load config for k values\n",
    "config = toml.load(\"../config.toml\")\n",
    "initial_k = config[\"SEARCH\"][\"initial_k\"]\n",
    "final_k = config[\"SEARCH\"][\"final_k\"]\n",
    "\n",
    "# Use shared function to calculate metrics\n",
    "# Returns both aggregated results and query-level metrics\n",
    "aggregated, query_metrics_df = calculate_evaluation_metrics(eval_df, initial_k, final_k)\n",
    "\n",
    "print(f\"Calculated metrics for {len(aggregated)} model configurations\")\n",
    "print(\"\\nSummary by model:\")\n",
    "aggregated[[\"bi_encoder_model\", \"db_tag\", \"meta_data_included\", \"query_location\", \"recall_at_initial_k\", \"recall_at_final_k\", \"mrr\"]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model labels for display\n",
    "def create_model_label(row):\n",
    "    model = row['bi_encoder_model'].split('/')[-1]  # Get short model name\n",
    "    meta = \"meta\" if row['meta_data_included'] else \"no-meta\"\n",
    "    tag = f\"-{row['db_tag']}\" if row['db_tag'] else \"\"\n",
    "    return f\"{model}{tag}\\n({meta})\"\n",
    "\n",
    "aggregated['model_label'] = aggregated.apply(create_model_label, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare Recall@K by model and location\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for idx, query_location in enumerate([False, True]):\n",
    "    loc_data = aggregated[aggregated['query_location'] == query_location].copy()\n",
    "    loc_data = loc_data.sort_values('recall_at_initial_k', ascending=False)\n",
    "    \n",
    "    ax = axes[idx]\n",
    "    bars = ax.barh(range(len(loc_data)), loc_data['recall_at_initial_k'])\n",
    "    ax.set_yticks(range(len(loc_data)))\n",
    "    ax.set_yticklabels(loc_data['model_label'], fontsize=9)\n",
    "    ax.set_xlabel('Recall@K (%)', fontsize=11)\n",
    "    ax.set_title(f\"Query with Location: {query_location}\", fontsize=12, fontweight='bold')\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, (bar, val) in enumerate(zip(bars, loc_data['recall_at_initial_k'])):\n",
    "        ax.text(val + 1, i, f'{val:.1f}%', va='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare MRR by model and location\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for idx, query_location in enumerate([False, True]):\n",
    "    loc_data = aggregated[aggregated['query_location'] == query_location].copy()\n",
    "    loc_data = loc_data.sort_values('mrr', ascending=False)\n",
    "    \n",
    "    ax = axes[idx]\n",
    "    bars = ax.barh(range(len(loc_data)), loc_data['mrr'], color='forestgreen')\n",
    "    ax.set_yticks(range(len(loc_data)))\n",
    "    ax.set_yticklabels(loc_data['model_label'], fontsize=9)\n",
    "    ax.set_xlabel('Mean Reciprocal Rank (MRR)', fontsize=11)\n",
    "    ax.set_title(f\"Query with Location: {query_location}\", fontsize=12, fontweight='bold')\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Add value labels on bars\n",
    "    for i, (bar, val) in enumerate(zip(bars, loc_data['mrr'])):\n",
    "        ax.text(val + 0.01, i, f'{val:.3f}', va='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare average rank overall (lower is better)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for idx, query_location in enumerate([False, True]):\n",
    "    loc_data = aggregated[aggregated['query_location'] == query_location].copy()\n",
    "    loc_data = loc_data.sort_values('avg_rank_overall', ascending=True)\n",
    "    \n",
    "    ax = axes[idx]\n",
    "    bars = ax.barh(range(len(loc_data)), loc_data['avg_rank_overall'], color='steelblue')\n",
    "    ax.set_yticks(range(len(loc_data)))\n",
    "    ax.set_yticklabels(loc_data['model_label'], fontsize=9)\n",
    "    ax.set_xlabel('Average Rank Overall', fontsize=11)\n",
    "    ax.set_title(f\"Query with Location: {query_location}\", fontsize=12, fontweight='bold')\n",
    "    ax.grid(axis='x', alpha=0.3)\n",
    "    \n",
    "    # Add value labels\n",
    "    for i, (bar, val) in enumerate(zip(bars, loc_data['avg_rank_overall'])):\n",
    "        ax.text(val + 2, i, f'{val:.1f}', va='center', fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary table: Compare models side by side\n",
    "summary = aggregated.pivot_table(\n",
    "    index=['bi_encoder_model', 'db_tag', 'meta_data_included'],\n",
    "    columns='query_location',\n",
    "    values=['recall_at_initial_k', 'recall_at_final_k', 'mrr', 'avg_rank_overall'],\n",
    "    aggfunc='mean'\n",
    ")\n",
    "\n",
    "# Flatten the MultiIndex columns for better readability\n",
    "summary.columns = ['_'.join(map(str, col)).strip() for col in summary.columns.values]\n",
    "summary = summary.reset_index()\n",
    "\n",
    "# Create a cleaner model identifier column\n",
    "summary['model_config'] = summary.apply(\n",
    "    lambda row: f\"{row['bi_encoder_model'].split('/')[-1]}{'-' + row['db_tag'] if row['db_tag'] else ''} ({'meta' if row['meta_data_included'] else 'no-meta'})\",\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Select and reorder columns for display\n",
    "display_cols = ['model_config'] + [col for col in summary.columns if col not in ['bi_encoder_model', 'db_tag', 'meta_data_included', 'model_config']]\n",
    "summary_display = summary[display_cols].copy()\n",
    "\n",
    "# Round numeric columns\n",
    "for col in summary_display.columns:\n",
    "    if summary_display[col].dtype in ['float64', 'int64']:\n",
    "        summary_display[col] = summary_display[col].round(2)\n",
    "\n",
    "# Display the cleaned DataFrame\n",
    "print(\"Summary Table: Metrics by Model and Location\")\n",
    "print(\"=\" * 100)\n",
    "display(summary_display)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query-Level Analysis\n",
    "\n",
    "Which queries are hardest? Which are easiest?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate by query to see which are hardest/easiest across all models\n",
    "# Note: recall_at_initial_k is already on a 0-100 scale from calculate_evaluation_metrics()\n",
    "query_summary = query_metrics_df.groupby(['query', 'query_location']).agg({\n",
    "    'recall_at_initial_k': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "# Show hardest queries (lowest success rate)\n",
    "print(\"Hardest Queries (lowest success rate across all models):\")\n",
    "print(\"-\" * 60)\n",
    "hardest = query_summary.nsmallest(10, 'recall_at_initial_k')\n",
    "for _, row in hardest.iterrows():\n",
    "    loc_str = \"with location\" if row['query_location'] else \"no location\"\n",
    "    print(f\"{row['query'][:60]:60s} ({loc_str}): {row['recall_at_initial_k']:.1f}%\")\n",
    "\n",
    "print(\"\\n\\nEasiest Queries (highest success rate across all models):\")\n",
    "print(\"-\" * 60)\n",
    "easiest = query_summary.nlargest(10, 'recall_at_initial_k')\n",
    "for _, row in easiest.iterrows():\n",
    "    loc_str = \"with location\" if row['query_location'] else \"no location\"\n",
    "    print(f\"{row['query'][:60]:60s} ({loc_str}): {row['recall_at_initial_k']:.1f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Location Impact Analysis\n",
    "\n",
    "Does adding location information help?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare location vs no-location for each model (Recall@K)\n",
    "location_impact_recall = aggregated.pivot_table(\n",
    "    index=['bi_encoder_model', 'db_tag', 'meta_data_included'],\n",
    "    columns='query_location',\n",
    "    values='recall_at_initial_k'\n",
    ").reset_index()\n",
    "location_impact_recall.columns = ['bi_encoder_model', 'db_tag', 'meta_data_included', 'no_location', 'with_location']\n",
    "location_impact_recall['improvement'] = location_impact_recall['with_location'] - location_impact_recall['no_location']\n",
    "location_impact_recall = location_impact_recall.sort_values('improvement', ascending=False)\n",
    "\n",
    "location_impact_recall['model_label'] = location_impact_recall.apply(create_model_label, axis=1)\n",
    "\n",
    "print(\"Location Impact on Recall@K (with_location - no_location):\")\n",
    "print(\"=\" * 80)\n",
    "print(location_impact_recall[['model_label', 'no_location', 'with_location', 'improvement']].to_string(index=False))\n",
    "\n",
    "# Visualize Recall@K\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "x = np.arange(len(location_impact_recall))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x - width/2, location_impact_recall['no_location'], width, label='No Location', alpha=0.8)\n",
    "bars2 = ax.bar(x + width/2, location_impact_recall['with_location'], width, label='With Location', alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Model Configuration', fontsize=11)\n",
    "ax.set_ylabel('Recall@K (%)', fontsize=11)\n",
    "ax.set_title('Impact of Adding Location Information (Recall@K)', fontsize=12, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(location_impact_recall['model_label'], rotation=45, ha='right', fontsize=9)\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare location vs no-location for each model (MRR)\n",
    "location_impact_mrr = aggregated.pivot_table(\n",
    "    index=['bi_encoder_model', 'db_tag', 'meta_data_included'],\n",
    "    columns='query_location',\n",
    "    values='mrr'\n",
    ").reset_index()\n",
    "location_impact_mrr.columns = ['bi_encoder_model', 'db_tag', 'meta_data_included', 'no_location', 'with_location']\n",
    "location_impact_mrr['improvement'] = location_impact_mrr['with_location'] - location_impact_mrr['no_location']\n",
    "location_impact_mrr = location_impact_mrr.sort_values('improvement', ascending=False)\n",
    "\n",
    "location_impact_mrr['model_label'] = location_impact_mrr.apply(create_model_label, axis=1)\n",
    "\n",
    "print(\"Location Impact on MRR (with_location - no_location):\")\n",
    "print(\"=\" * 80)\n",
    "print(location_impact_mrr[['model_label', 'no_location', 'with_location', 'improvement']].to_string(index=False))\n",
    "\n",
    "# Visualize MRR\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "x = np.arange(len(location_impact_mrr))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x - width/2, location_impact_mrr['no_location'], width, label='No Location', alpha=0.8)\n",
    "bars2 = ax.bar(x + width/2, location_impact_mrr['with_location'], width, label='With Location', alpha=0.8)\n",
    "\n",
    "ax.set_xlabel('Model Configuration', fontsize=11)\n",
    "ax.set_ylabel('Mean Reciprocal Rank (MRR)', fontsize=11)\n",
    "ax.set_title('Impact of Adding Location Information (MRR)', fontsize=12, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(location_impact_mrr['model_label'], rotation=45, ha='right', fontsize=9)\n",
    "ax.legend()\n",
    "ax.grid(axis='y', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Average Rank vs MMR: \n",
    "Average Rank: includes all queries. If the answer isn’t found, it assigns initial_k + 1 (201). So it heavily penalizes misses.\n",
    "\n",
    "MRR: for misses, it contributes 0 (doesn’t penalize). For hits, it uses 1/rank, heavily favoring top ranks.\n",
    "\n",
    "So a model can have:\n",
    "Lower average rank (better) because it finds answers more often, even at mediocre ranks\n",
    "Lower MRR (worse) because when it finds answers, they’re not at top ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (slayer_search)",
   "language": "python",
   "name": "slayer_search"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
